{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_answer=\"London is the capital city America rebuttal of England\".lower() \n",
    "original_candidate_answer=\"London is the capital city USA of England\".lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the stopwords in english\n",
    "stop_words=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize model and candiadte answer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "model_answer_tokenized = word_tokenize(original_model_answer)\n",
    "candidate_answer_tokenized = word_tokenize(original_candidate_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words from model and candiadte answer\n",
    "def removing_stopwords(tokenized_answer):\n",
    "    filtered_sentence=[]\n",
    "    for word in tokenized_answer:\n",
    "         if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['london', 'capital', 'city', 'usa', 'england'] ['london', 'capital', 'city', 'america', 'rebuttal', 'england']\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords from both model and candidate answer using above created function\n",
    "model_answer=removing_stopwords(model_answer_tokenized)\n",
    "candidate_answer=removing_stopwords(candidate_answer_tokenized)\n",
    "print(candidate_answer,model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting percentage of proper nouns matched \n",
    "def proper_noun_match_percentage(candidate_answer,model_answer):\n",
    "    #pos tagging both answers\n",
    "    candidate_answer_POS=nltk.pos_tag(candidate_answer)\n",
    "    model_answer_POS=nltk.pos_tag(model_answer)\n",
    "    \n",
    "    #function to get proper nouns from an answer\n",
    "    def getting_proper_noun(answer):\n",
    "        proper_nouns=[]\n",
    "        for word in answer:\n",
    "            if word[1] == 'NNP':\n",
    "                proper_nouns.append(word[0])\n",
    "        return proper_nouns\n",
    "    \n",
    "    #calling above defined function to get proper nouns in both candidate and model answer\n",
    "    candidate_answer_proper_nouns = getting_proper_noun(candidate_answer_POS)\n",
    "    model_answer_proper_nouns = getting_proper_noun(model_answer_POS)\n",
    "    \n",
    "    #checking the number of proper nouns in model answer\n",
    "    proper_noun_count = len(model_answer_proper_nouns)\n",
    "    \n",
    "    #variable that will count the number of proper nouns that the candidate answer has which is also available in model answer\n",
    "    proper_noun_match_count = 0\n",
    "    \n",
    "    #finding the number of proper nouns that match\n",
    "    for word in candidate_answer_proper_nouns:\n",
    "        if word in model_answer_proper_nouns:\n",
    "            proper_noun_match_count += 1\n",
    "    \n",
    "    #calculating percentage of proper nouns that are in candidate answer comapre to the number of proper nouns in the model answer\n",
    "    proper_noun_match_percentage = ( proper_noun_match_count / proper_noun_count) * 100\n",
    "    \n",
    "    return proper_noun_match_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "#calculating percentage of words that match\n",
    "def word_match_percentage(candidate_answer,model_answer):\n",
    "    unmatched_words=[]\n",
    "    matched_words_count=0\n",
    "    \n",
    "    #checks for word in candidate answer that matches with model answer and increase matched_words_count. The words that don't match are inserted into unmatched_words list \n",
    "    for word in model_answer:\n",
    "        if word in candidate_answer:\n",
    "            matched_words_count += 1\n",
    "        else:\n",
    "            unmatched_words.append(word)\n",
    "\n",
    "    #find synonyms for unmatched words\n",
    "    unmatched_words_synonyms = []\n",
    "    for word in unmatched_words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                unmatched_words_synonyms.append(l.name())\n",
    "    \n",
    "    #checking if the synonyms of unmatched words match with words in candidate answer, and is it does increase matched_words_count\n",
    "    for word in unmatched_words_synonyms:\n",
    "        if word in candidate_answer:\n",
    "            matched_words_count += 1\n",
    "            \n",
    "    #finding the precentage of similair words in candidate answer compared to model answer        \n",
    "    model_answer_length = len(model_answer)\n",
    "    word_match_percentage = ( matched_words_count / model_answer_length) * 100\n",
    "    \n",
    "    return word_match_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "#Bigram similarity\n",
    "def bigram_similarity_percentage(candidate_answer,model_answer):\n",
    "    candidate_answer_bigram_tokenized = []\n",
    "    model_answer_bigram_tokenized = []\n",
    "    \n",
    "    #getting bigrams of both model and candidate answer\n",
    "    candidate_bigram = bigrams(candidate_answer)\n",
    "    model_bigram = bigrams(model_answer)\n",
    "    \n",
    "    for w in candidate_bigram:\n",
    "        candidate_answer_bigram_tokenized.append(w)\n",
    "        \n",
    "    for w in model_bigram:\n",
    "        model_answer_bigram_tokenized.append(w)\n",
    "\n",
    "    \n",
    "    #checking the number of bigrams in model answer\n",
    "    bigrams_count = len(model_answer_bigram_tokenized)\n",
    "    \n",
    "    #variable that will count the number of bigrams that the candidate answer has which is also available in model answer\n",
    "    bigram_similarity_count = 0\n",
    "    \n",
    "    #finding the number of bigrams that match\n",
    "    for bigram in candidate_answer_bigram_tokenized:\n",
    "        if bigram in model_answer_bigram_tokenized:\n",
    "            bigram_similarity_count += 1\n",
    "    \n",
    "    #calculating percentage of bigrams that are in candidate answer comapred to the number of bigrams in the model answer\n",
    "    bigram_similarity_percentage = ( bigram_similarity_count / bigrams_count) * 100\n",
    "    \n",
    "    return bigram_similarity_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity\n",
    "def cosine_similarity(candidate_answer,model_answer):\n",
    "\n",
    "    # form a list containing keywords of both strings. Basically union of candidate_answer and model_answer  \n",
    "    keywords_union = list(set(candidate_answer) | set(model_answer))\n",
    "    A =[];B =[] \n",
    "    for w in keywords_union: \n",
    "        if w in candidate_answer: A.append(1) # create a vector \n",
    "        else: A.append(0) \n",
    "        if w in model_answer: B.append(1) \n",
    "        else: B.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(keywords_union)): \n",
    "            c+= A[i]*B[i] \n",
    "    cosine = c / float((sum(A)*sum(B))**0.5) \n",
    "    print(\"similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jaccard similarity\n",
    "def jaccard_similarity(candidate_answer,model_answer):\n",
    "    \n",
    "    #getting intersection of candidate_answer and model_answer\n",
    "    keywords_intersection = list(set(candidate_answer) & set(model_answer)) \n",
    "    \n",
    "    #form a list containing keywords of both strings. Basically union of candidate_answer and model_answer  \n",
    "    keywords_union = list(set(candidate_answer) | set(model_answer))\n",
    "    \n",
    "    return (len(keywords_intersection) / len(keywords_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dice similarity\n",
    "def dice_similarity(candidate_answer,model_answer):\n",
    "    \n",
    "    #getting intersection of candidate_answer and model_answer\n",
    "    keywords_intersection = list(set(candidate_answer) & set(model_answer))\n",
    "    \n",
    "    return ((2 * len(keywords_intersection)) / (len(candidate_answer) + len(model_answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
